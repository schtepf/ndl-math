\pdfminorversion=4 % avoid compatibility problems with embedded PNGs in Adobe Acrobat
%% \documentclass[handout,t]{beamer} % HANDOUT
%% \documentclass[handout,notes=show,t]{beamer} % NOTES
\documentclass[t]{beamer} % SLIDES
\usepackage{etex}

\usetheme{StefanAntti}                % page themes: StefanPlain, StefanFAU, StefanOsna
%% \usepackage{beamer-font-lucida}  % commercial Lucida Math font package (recommended)
%% \usepackage{beamer-font-charter} % commercial Charter Match font package
%% \usepackage{beamer-font-arev}    % use Arev fonts from TexLive (combine with document option "smaller")
\usepackage{beamer-tools-stefan}    % standard packages, definitions and macros

%%%% uncomment the following macro libraries as needed
%% \input{lib/pgf}   % basic PGF utility functions
%% \input{lib/text}  % some macros for typesetting text
\input{lib/math}  % basic mathematical notation
\input{lib/stat}  % notation for probability theory and statistics
\input{lib/vector}% convenience macros for vectors and matrices
%% \input{lib/grid}  % grid-like layout of text and graphics in PGF picture
%% \input{lib/tree}  % typesetting parse trees with PGF
%% \input{lib/chart} % demonstrating chart parsers with PGF
%% \input{lib/fl}    % some notation for formal language theory

\input{local/config} % local adjustments to configuration and macros

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Titlepage

\title[NDL: Theory \& Experiments]{\textbf{Naïve Discriminative Learning:}\\
  Theoretical and Experimental Observations}
%%\subtitle{}
\author[S.~Evert \& A.~Arppe]{Stefan Evert$^1$ \& Antti Arppe$^2$}
\institute[]{%
  $^1$Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany\\
  {\secondary{\url{stefan.evert@fau.de}}}\\[1em]
  $^2$University of Alberta, Edmonton, Canada\\
  {\secondary{\url{arppe@ualberta.ca}}}
}
\date[Tübingen, 6 Nov 2015]{QITL-6, Tübingen, 6 Nov 2015}


\begin{document}

%% \showLogo  % uncomment to show logo on title page in StefanPlain style
\frame{\titlepage}
\hideLogo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Outline

\section*{Outline}
\frame{ 
  \frametitle{Outline}
  \tableofcontents
}
\AtBeginSubsection[]
{
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[current,currentsubsection]
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 1 Introduction
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Naïve Discriminative Learning}

\begin{frame}
  \frametitle{Objectives}

  \begin{itemize}
  \item Explain the mathematical foundations of Naïve Discriminative Learning (NDL) in one place and in a consistent way
  \item Highlight the theoretical similarities of NDL with linear/logistic regression and the single-layer perceptron
  \item Present some empirical simulations of stochastic NDL learners, in light of the theoretical insights
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Naïve Discriminative Learning}
  
  \begin{itemize}
  \item \citet{Baayen:11,Baayen:etc:11}
  \item Incremental learning equations for direct associations between cues and outcomes \citep{Rescorla:Wagner:72} 
  \item Equilibrium conditions \citep{Danks:03}
  \item Implementation as R package \texttt{ndl} \citep{Arppe:etc:14}
  \end{itemize}
  
  \gap[1]
  \begin{description}[Discriminative:]
   \item[Naive:] cue-outcome associations estimated separately for
    each outcome (this independence assumption is similar to
    a naive Bayesian classifier)
  \item[Discriminative:] cues predict outcomes based on total activation level
    = sum of direct cue-outcome associations
  \item[Learning:] incremental learning of association strengths
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{The Rescorla-Wagner equations (1972)}

  Represent incremental associative learning and subsequent on-going
  adjustments to an accumulating body of knowledge.

  \gap[1]
  Changes in cue-outcome association strengths:
  \begin{itemize}
  \item No change if a cue is not present in the input
  \item Increased if the cue and outcome co-occur
  \item Decreased if the cue occurs without the outcome
  \item If outcome can already be predicted well (based on all input cues),
    adjustments become smaller
  \end{itemize}

  \gap[1] 
  Only results of incremental adjustments to the cue-outcome associations are
  kept -- no need for remembering the individual adjustments, however many
  there are.
\end{frame}

\begin{frame}
  \frametitle{Danks (2003) equilibrium conditions} 

  \begin{itemize}
  \item Presume an ideal stable ``adult'' state, where all cue-outcome
    associations have been fully learnt -- further data points should then
    have no impact on the cue-outcome associations
  \item Provide a convenient short-cut to calculating the final cue-outcome
    association weights resulting from incremental learning, using relatively
    simple matrix algebra
  \item Most learning parameters of the Rescorla-Wagner equations drop
    out of the Danks equilibrium equation
  \item Circumvent the problem that a simulation of an R-W learner does
    usually not converge to a stable state unless the learning rate is
    gradually decreased
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Traditional \vs linguistic applications of R-W}

  \begin{itemize}
  \item Traditionally: simple controlled experiments on item-by-item
    learning, with only a handful of cues and perfect associations
  \item Natural language: full of choices among multiple possible alternatives
    -- phones, words, or constructions -- which are influenced by a large
    number of contextual factors, and which often show weak to moderate 
    tendencies towards one or more of the alternatives rather than a
    single unambiguous decision
  \item These messy, complex types of problems are a key area of interest in
    modeling and understanding language use
  \item Application of R-W in the form of a Naïve Discriminative Learner to
    such linguistic classification problems is sucessful in practice and can 
    throw new light on research questions
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Related work}

  \begin{itemize}
  \item R-W \vs perceptron \citep[p.~155f]{Sutton:Barto:81}
  \item R-W \vs least-squares regression \citep[p.~457]{Stone:86}
  \item R-W \vs logistic regression \citep[p.~234]{Gluck:Bower:88}
  \item R-W \vs neural networks \citep{Dawson:08}
  \item[\hand] similarities are also mentioned by many other authors \ldots
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{An example}

\begin{frame}
  \frametitle{Simple \vs complex settings -- QITL-1 revisited}

  \begin{itemize}
  \item \citet{Arppe:Jarvikivi:02,Arppe:Jarvikivi:07}
  \item \textit{Person} (\textsc{first person singular} or not) and
    \textit{Countability} (\textsc{collective} or not) of \textsc{agent/subject} of
    Finnish verb synonym pair \textit{mietti{\"a}} \vs \textit{pohtia}
    `think, ponder':
  \end{itemize}

  \begin{center}\scriptsize
    \begin{tabular}{ c  c || c || c  c}
      \hline
      \multicolumn{2}{c||}{Forced-choice}                     & Frequency           & \multicolumn{2}{c}{Acceptability}    \\
      Dispreferred                          & Preferred       & (relative)          & Unacceptable                          & Acceptable       \\ \hline \hline
      \multicolumn{1}{c|}{$\varnothing$}    & mietti{\"a}+SG1 & Frequent            & \multicolumn{1}{c|}{$\varnothing$}    & mietti{\"a}+SG1  \\
      \multicolumn{1}{c|}{}                 & pohtia+COLL     &                     & \multicolumn{1}{c|}{}                 & pohtia{\"a}+COLL \\ \hline
      \multicolumn{1}{c|}{mietti{\"a}+COLL} &                 &                     & \multicolumn{1}{c|}{}                 &                  \\
      \multicolumn{1}{c|}{pohtia+SG1}       & $\varnothing$   & Rare                & \multicolumn{1}{c|}{mietti{\"a}+COLL} & pohtia+SG1       \\
      \hline
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{QITL-1 through the lens of NDL}

  \centering\ungap[1]
  \only<beamer:1| handout:0>{\includegraphics[width=8cm]{{{img/think.qitl1.AgentGroup_pohtia_RW_vs_D}}}}%
  \only<beamer:2| handout:0>{\includegraphics[width=8cm]{{{img/think.qitl1.AgentGroup_miettia_RW_vs_D}}}}%
  \only<beamer:3| handout:0>{\includegraphics[width=8cm]{{{img/think.qitl1.PersonFirst_pohtia_RW_vs_D}}}}%
  \only<beamer:4| handout:0>{\includegraphics[width=8cm]{{{img/think.qitl1.PersonFirst_miettia_RW_vs_D}}}}%
  \only<beamer:0| handout:1>{%
    \begin{tabular}{cc}
      \includegraphics[width=4cm]{{{img/think.qitl1.AgentGroup_pohtia_RW_vs_D}}} &
      \includegraphics[width=4cm]{{{img/think.qitl1.AgentGroup_miettia_RW_vs_D}}} \\
      \includegraphics[width=4cm]{{{img/think.qitl1.PersonFirst_pohtia_RW_vs_D}}} &
      \includegraphics[width=4cm]{{{img/think.qitl1.PersonFirst_miettia_RW_vs_D}}} \\
    \end{tabular}}%
\end{frame}

\begin{frame}[c]
  \frametitle{QITL-1 through the lens of QITL-6}
  \framesubtitle{(courtesy of Dagmar Divjak)}

  \centering\ungap[2]
  \includegraphics[width=8cm]{{{img/TRY.ACCEPTABILITY_vs_Probability}}}
\end{frame}

\begin{frame}[c]
  \frametitle{Simple \vs complex settings -- QITL-2 revisited}

  \centering
  \includegraphics[width=7cm]{{{img/THINK.maximal_linguistic_variable_density}}}
\end{frame}

\begin{frame}
  \frametitle{QITL-4 revisited -- NDL \vs statistical classifiers}

  \begin{center}
    \footnotesize
    \begin{tabular}{lrrr}
      \hline
      & $\lambda_{\mbox{\tiny prediction}}$ & $\tau_{\mbox{\tiny classification}}$ & accuracy \\ 
      \hline
      Polytomous logistic regression & 0.368 & 0.488 & \textbf{0.645} \\
      (One-vs-rest) &  &  &  \\ 
      Polytomous mixed logistic regression &  &  &  \\ 
      (Poisson reformulation) &  &  &  \\ 
      $\quad\bullet$ 1$|$Section & 0.360 & 0.482 & 0.640 \\
      $\quad\bullet$ 1$|$Author & 0.358 & 0.481 & 0.640 \\
      $\quad\bullet$ 1$|$Section + 1$|$Author & 0.358 & 0.481 & 0.640 \\
      Support Vector Machine & 0.340 & 0.466 & 0.629 \\
      Memory-Based Learning & 0.286 & 0.422 & 0.599 \\
      (TiMBL) &  &  &  \\
      Random Forests & 0.326 & 0.455 & 0.621 \\
      Naive Discriminative Learning & 0.346 & 0.471 & \primary{0.632} \\
      \hline
    \end{tabular}
  \end{center}
  
  \scriptsize
  \secondary{Table:} Classification diagnostics for models fitted to the Finnish data set ($n=3404$). 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 2 Mathematics
\section{Mathematics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Rescorla-Wagner equations}

\begin{frame}
  \frametitle{The Rescorla-Wagner equations}
  %% \framesubtitle{}

  \begin{itemize}
  \item<1-> Goal of naïve discriminative learner: predict an \primary{outcome} $O$ based on presence or absence of a set of \primary{cues} $C_1, \ldots, C_n$
  \item<2-> An \primary{event} $(\vc, o)$ is formally described by indicator variables
    \begin{align*}
      c_i &= 
       \begin{cases}
         1 & \text{if $C_i$ is present} \\
         0 & \text{otherwise}
       \end{cases}
      &
      o &= 
       \begin{cases}
         1 & \text{if $O$ results} \\
         0 & \text{otherwise}
       \end{cases}
    \end{align*}
  \item<3-> Given cue-outcome \primary{associations} $\vv = (V_1, \ldots, V_n)$ of learner, the \primary{activation level} of the outcome $O$ is
    \[
    \only<beamer:3| handout:1>{
      \sum_{j=1}^n c_j V_j}
    \only<beamer:4-| handout:0>{
      \sum_{j=1}^n c_j\psupt V_j\psupt}
    \]
  \item<4-> Associations $\vv[t]$ as well as cue and outcome indicators $(\vc[t], o\psupt)$ depend on time step $t$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Rescorla-Wagner equations}
  %% \framesubtitle{}
  
  \begin{itemize}
  \item \citet{Rescorla:Wagner:72} proposed the \primary{R-W equations} for the change in associations given an event $(\vc, o)$:
    \[
    \Delta V_i =
    \begin{cases}
      0 & \text{if } c_i = 0\\
      \alpha_i \beta_1 \bigl(\lambda - \sum_{j=1}^n c_j V_j \bigr) & \text{if } c_i = 1 \wedge o = 1 \\
      \alpha_i \beta_2 \bigl(0 - \sum_{j=1}^n c_j V_j \bigr) & \text{if } c_i = 1 \wedge o = 0 
    \end{cases}
    \]
    with parameters
    \ungap[.5]
    \begin{align*}
      \lambda &> 0   && \text{target activation level for outcome $O$} \\
      \alpha_i &> 0  && \text{salience of cue $C_i$} \\
      \beta_1 &> 0   && \text{learning rate for positive ovents ($o = 1$)} \\
      \beta_2 &> 0   && \text{learning rate for negative ovents ($o = 0$)}
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}<beamer:1-3| handout:1>
  \frametitle{The Widrow-Hoff rule}
  %% \framesubtitle{}
  
  \begin{itemize}
  \item The \primary{W-H rule} \citep{Widrow:Hoff:60} is a widely-used simplification of the R-W equations:
    \begin{align*}
    \Delta V_i &=
    \begin{cases}
      0 & \text{if } c_i = 0\\
      \only<beamer:1| handout:0>{\alpha_i} \beta\only<beamer:1| handout:0>{_1} \bigl(\only<beamer:1| handout:0>{\lambda}\only<beamer:2-| handout:1>{1} - \sum_{j=1}^n c_j V_j \bigr) & \text{if } c_i = 1 \wedge o = 1 \\
      \only<beamer:1| handout:0>{\alpha_i} \beta\only<beamer:1| handout:0>{_2} \bigl(0 - \sum_{j=1}^n c_j V_j \bigr) & \text{if } c_i = 1 \wedge o = 0 
    \end{cases}
    \only<beamer:3-| handout:1>{\\
       &= \primary{c_i \beta \bigl( o - \textstyle\sum_{j=1}^n c_j V_j \bigr)}}
    \end{align*}
    with parameters
    \ungap[.5]
    \begin{align*}
      \lambda  &= 1   && \text{target activation level for outcome $O$} \\
      \alpha_i &= 1  && \text{salience of cue $C_i$} \\
      \beta_1  &= \beta_2   && \text{global learning rate for positive and}\\
               &= \beta > 0 && \text{negative events}
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{A simple example: German noun plurals}
  %% \framesubtitle{}
  
  \small\centering
  \begin{tabular}{r>{\color{counterpoint}}l|c|cccccc}
    \toprule
    $t$ & & $o$ & $c_1$ & $c_2$ & $c_3$ & $c_4$ & $c_5$ & $c_6$ \\
    & \secondary{word} & \secondary{pl?} & \secondary{\emph{--e}} & \secondary{\emph{--n}} & \secondary{\emph{--s}} & \secondary{umlaut} & \secondary{dbl cons} & \secondary{bgrd}\\
    \midrule
    1 &   Bäume &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\ 
    2 & Flasche &  0  &  1 & 0 & 0 & 0 & 0 & 1 \\ 
    3 &    Baum &  0  &  0 & 0 & 0 & 0 & 0 & 1 \\ 
    4 &  Gläser &  1  &  0 & 0 & 0 & 1 & 0 & 1 \\ 
    5 &Flaschen &  1  &  0 & 1 & 0 & 0 & 0 & 1 \\ 
    6 &   Latte &  0  &  1 & 0 & 0 & 0 & 1 & 1 \\ 
    7 &  Hütten &  1  &  0 & 1 & 0 & 1 & 1 & 1 \\ 
    8 &    Glas &  0  &  0 & 0 & 1 & 0 & 0 & 1 \\ 
    9 &   Bäume &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\ 
   10 &    Füße &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\
    \bottomrule
  \end{tabular}
\end{frame}

\begin{frame}[c]
  \frametitle{A simple example: German noun plurals}
  %% \framesubtitle{}
  
  \footnotesize\centering
  \begin{tabular}{>{\color{counterpoint}}c|c|cccccc}
    \toprule
    \secondary{$t$} & \secondary{$\sum c_j V_j$} & \secondary{$V_1$} & \secondary{$V_2$} & \secondary{$V_3$} & \secondary{$V_4$} & \secondary{$V_5$} & \secondary{$V_6$} \\
\only<beamer:1| handout:0>{\color{foreground}  1 & .000 & .000 & .000 &  .000 & .000 &  .000 & .000 }% 
\only<beamer:2| handout:0>{\color{foreground}  2 & .400 & .200 & .000 &  .000 & .200 &  .000 & .200 }% 
\only<beamer:3| handout:0>{\color{foreground}  3 & .120 & .120 & .000 &  .000 & .200 &  .000 & .120 }% 
\only<beamer:4| handout:0>{\color{foreground}  4 & .296 & .120 & .000 &  .000 & .200 &  .000 & .096 }% 
\only<beamer:5| handout:0>{\color{foreground}  5 & .237 & .120 & .000 &  .000 & .341 &  .000 & .237 }% 
\only<beamer:6| handout:0>{\color{foreground}  6 & .509 & .120 & .153 &  .000 & .341 &  .000 & .389 }% 
\only<beamer:7| handout:0>{\color{foreground}  7 & .679 & .018 & .153 &  .000 & .341 & -.102 & .288 }% 
\only<beamer:8| handout:0>{\color{foreground}  8 & .352 & .018 & .217 &  .000 & .405 & -.038 & .352 }% 
\only<beamer:9| handout:0>{\color{foreground}  9 & .704 & .018 & .217 & -.070 & .405 & -.038 & .281 }% 
\only<beamer:10| handout:1>{\color{foreground}10 & .882 & .077 & .217 & -.070 & .464 & -.038 & .340 }% 
\only<beamer:11| handout:0>{\color{foreground}11 &      & .101 & .217 & -.070 & .488 & -.038 & .364 }%
    \\
    \midrule
    \only<beamer:1| handout:0>{   Bäume &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\ }% 
    \only<beamer:2| handout:0>{ Flasche &  0  &  1 & 0 & 0 & 0 & 0 & 1 \\ }% 
    \only<beamer:3| handout:0>{    Baum &  0  &  0 & 0 & 0 & 0 & 0 & 1 \\ }% 
    \only<beamer:4| handout:0>{  Gläser &  1  &  0 & 0 & 0 & 1 & 0 & 1 \\ }% 
    \only<beamer:5| handout:0>{Flaschen &  1  &  0 & 1 & 0 & 0 & 0 & 1 \\ }% 
    \only<beamer:6| handout:0>{   Latte &  0  &  1 & 0 & 0 & 0 & 1 & 1 \\ }% 
    \only<beamer:7| handout:0>{  Hütten &  1  &  0 & 1 & 0 & 1 & 1 & 1 \\ }% 
    \only<beamer:8| handout:0>{    Glas &  0  &  0 & 0 & 1 & 0 & 0 & 1 \\ }% 
    \only<beamer:9| handout:0>{   Bäume &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\ }% 
    \only<beamer:10| handout:1>{   Füße &  1  &  1 & 0 & 0 & 1 & 0 & 1 \\}% 
    \only<beamer:11| handout:0>{& & & & & & & \\ }%
    \phantom{Flaschen} & \secondary{$o$} & \secondary{$c_1$} & \secondary{$c_2$} & \secondary{$c_3$} & \secondary{$c_4$} & \secondary{$c_5$} & \secondary{$c_6$} \\
    \bottomrule
  \end{tabular}

  \gap[1]
  \only<beamer:1| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_1}}%
  \only<beamer:2| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_2}}%
  \only<beamer:3| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_3}}%
  \only<beamer:4| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_4}}%
  \only<beamer:5| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_5}}%
  \only<beamer:6| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_6}}%
  \only<beamer:7| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_7}}%
  \only<beamer:8| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_8}}%
  \only<beamer:9| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_9}}%
  \only<beamer:10| handout:1>{\includegraphics[width=8cm]{img/german_plural_rw_step_10}}%
  \only<beamer:11| handout:0>{\includegraphics[width=8cm]{img/german_plural_rw_step_11}}%
\end{frame}

\begin{frame}
  \frametitle{A stochastic NDL learner}
  %% \framesubtitle{}

  \begin{itemize}
  \item<1-> A specific event sequence $(\vc[t], o\psupt)$ will only be encountered in controlled experiments
  \item<2-> For applications in corpus linguistics, it is more plausible to assume that events are randomly sampled from a population of \primary{event tokens} $(\vc[k], o\psup{k})$ for $k = 1, \ldots, m$
    \begin{itemize}
    \item[\hand] event types listed repeatedly proportional to their frequency
    \end{itemize}
  \item<3-> I.i.d.\ random variables $\vc[t] \sim \vc$ and $o\psupt\sim o$
    \begin{itemize}
    \item[\hand] distributions of $\vc$ and $o$ determined by population
    \end{itemize}
  \item<3-> NDL can now be trained for arbitrary number of time steps, even if population is small (as in our example)
    \begin{itemize}
    \item study asymptotic behaviour of learners
    \item convergence \so stable ``adult'' state of associations
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{A stochastic NDL learner}
  \framesubtitle{Effect of the learning rate $\beta$}

  \centering\ungap[1]
  \only<beamer:1| handout:1>{\includegraphics[width=11cm]{img/german_plural_rw_b050_n200}}%
  \only<beamer:2| handout:0>{\includegraphics[width=11cm]{img/german_plural_rw_b020_n200}}%
  \only<beamer:3| handout:0>{\includegraphics[width=11cm]{img/german_plural_rw_b010_n200}}%
  \only<beamer:4| handout:2>{\includegraphics[width=11cm]{img/german_plural_rw_b005_n200}}%
  \only<beamer:5| handout:0>{\includegraphics[width=11cm]{img/german_plural_rw_b001_n200}}%
  \only<beamer:6| handout:0>{\includegraphics[width=11cm]{img/german_plural_rw_b001_n2000}}%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{The Danks equilibrium}

\begin{frame}
  \frametitle{Expected activation levels}
  %% \framesubtitle{}

  \begin{itemize}
  \item Since we are interested in the general behaviour of a stochastic NDL, it makes sense to average over many individual learners to obtain \primary{expected associations} $\bigExp{V_j\psupt}$
  \end{itemize}

  \[
  \bigExp{V_{j}\psup{t+1}} = \bigExp{V_j\psupt} + \bigExp{\Delta V_j\psupt}
  \]

  \ungap[.5]
  \begin{align*}
    \bigExp{\Delta V_j\psupt} 
    &= \Expscale{ 
      c_i \beta \bigl( o - \textstyle\sum_{j=1}^n c_j V_j\psupt \bigr)
      } \qquad\qquad\qquad\qquad \\
    & \only<beamer:2| handout:0>{
      = \beta\cdot \bigExp{c_i o} - \beta\cdot \Expscale{c_i \textstyle\sum_{j=1}^n c_j V_j\psupt}
      }%
      \only<beamer:3| handout:0>{
      = \beta\cdot \bigExp{c_i o} - \beta\cdot \textstyle\sum_{j=1}^n \secondary{\bigExp{c_i c_j V_j\psupt}}
      }%
      \only<beamer:4| handout:0>{
      = \beta\cdot \secondary{\bigExp{c_i o}} - \beta\cdot \textstyle\sum_{j=1}^n \secondary{\bigExp{c_i c_j}} \bigExp{V_j\psupt}
      }% 
      \only<beamer:5| handout:1>{
      = \beta\cdot\left( \p{C_i, O} - \textstyle\sum_{j=1}^n \p{C_i, C_j} \bigExp{V_j\psupt} \right)
      }% 
  \end{align*}

  \begin{itemize}
  \item<3-> $c_i$ and $c_j$ are independent from $V_j\psupt$
  \item<4-> indicator variables: $\Exp{c_i o} = \p{C_i, O}$; $\Exp{c_i c_j} = \p{C_i, C_j}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Expected activation levels}
  %% \framesubtitle{}
  
  \ungap[1.5]
  \[
  \only<beamer:1-5| handout:0>{\Delta V_j\psupt = c_i\psupt \beta \bigl( o\psupt - \textstyle\sum_{j=1}^n c_j\psupt V_j\psupt \bigr)}%
  \only<beamer:6-| handout:1>{\bigExp{\Delta V_j\psupt} = \beta\cdot\bigl( \p{C_i, O} - \textstyle\sum_{j=1}^n \p{C_i, C_j} \bigExp{V_j\psupt} \bigr)}%
  \]
  
  \centering
  \only<beamer:1| handout:0>{\includegraphics[width=10cm]{img/german_plural_exp_rw_step_1}}%
  \only<beamer:2| handout:0>{\includegraphics[width=10cm]{img/german_plural_exp_rw_step_2}}%
  \only<beamer:3| handout:0>{\includegraphics[width=10cm]{img/german_plural_exp_rw_step_3}}%
  \only<beamer:4| handout:0>{\includegraphics[width=10cm]{img/german_plural_exp_rw_step_4}}%
  \only<beamer:5| handout:0>{\includegraphics[width=10cm]{img/german_plural_exp_rw_step_5}}%
  \only<beamer:6| handout:1>{\includegraphics[width=10cm]{img/german_plural_exp_rw_final}}%
\end{frame}

\begin{frame}
  \frametitle{The Danks equilibrium}
  %% \framesubtitle{}

  \begin{itemize}
  \item If $\bigExp{V_i\psupt}$ converges, the asymptote $V_i^* = \lim_{t\to \infty} \bigExp{V_i\psupt}$ must satisfy the \primary{Danks equilibrium} conditions $\bigExp{\Delta V_i^*} = 0$, i.e.
    \[
    \p{C_i, O} - \textstyle\sum_{j=1}^n \p{C_i, C_j} V_j^* = 0 \quad \forall i
    \]
    \citep[p.~113]{Danks:03}
    \begin{itemize}
    \item[]
    \end{itemize}
  \item Now there is a clear interpretation of the Danks equilibrium as the stable average associations reached by a community of stochastic learners with input from the same population
    \begin{itemize}
    \item[\hand] allows us to compute the ``adult'' state of NDL without carrying out a simulation of the learning process
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{The Danks equilibrium}
  %% \framesubtitle{}

  \centering
  \only<beamer:1| handout:0>{\includegraphics[width=11cm]{img/german_plural_exp_rw_danks}}%
  \only<beamer:2| handout:1>{\includegraphics[width=11cm]{img/german_plural_exp_rw_danks_500}}%
\end{frame}

\begin{frame}
  \frametitle{Matrix notation}
  %% \framesubtitle{}
  
  \ungap[2]
  \begin{align*}
    \vX &=
    \begin{bmatrix}
      c_1\psup{1} & \cdots & c_n\psup{1} \\
      c_1\psup{2} & \cdots & c_n\psup{2} \\
      \vdots      &        & \vdots      \\
      c_1\psup{m} & \cdots & c_n\psup{m} 
    \end{bmatrix}
    &
    \vz &=
    \begin{bmatrix}
      o\psup{1} \\
      o\psup{2} \\
      \vdots \\
      o\psup{m}
    \end{bmatrix}
    &
    \vw &=
    \begin{bmatrix}
      V\psup{1} \\
      \vdots \\
      V\psup{n}
    \end{bmatrix}
  \end{align*}
  
  \begin{align*}
    \only<beamer:2-3| handout:0>{
    \small\begin{bmatrix} 
      f(C_1, O) \\ 
      \vdots \\
      f(C_n, O) 
    \end{bmatrix}
    &= \vX^T \vz
    &
    \visible<3->{
    \small\begin{bmatrix} 
      f(C_1, C_1) & \cdots & f(C_1, C_n) \\ 
      \vdots      &       & \vdots \\
      f(C_n, C_1) & \cdots & f(C_n, C_n)
    \end{bmatrix}
    &= \vX^T \vX
    }}%
    \only<beamer:4-| handout:1>{
    \small\begin{bmatrix} 
      \p{C_1, O} \\ 
      \vdots \\
      \p{C_n, O}
    \end{bmatrix}
    &= \tfrac{1}{m} \vX^T \vz
    &
    \small\begin{bmatrix} 
      \p{C_1, C_1} & \cdots & \p{C_1, C_n} \\ 
      \vdots      &       & \vdots \\
      \p{C_n, C_1} & \cdots & \p{C_n, C_n}
    \end{bmatrix}
    &= \tfrac{1}{m} \vX^T \vX
    }
  \end{align*}
  
  \gap[1]
  \[
  \visible<5->{\text{\primary{Danks equilibrium:}}} \quad
  \only<beamer:5| handout:0>{\tfrac{1}{m} \vX^T \vz - \tfrac{1}{m} \vX^T \vX \vw^* = \vnull}%
  \only<beamer:6-| handout:1>{\vX^T \vz = \vX^T \vX \vw^*}%
  \]
\end{frame}

\begin{frame}
  \frametitle{Matrix notation: German noun plurals}
  %% \framesubtitle{}
  
  \ungap[2]
  \begin{align*}
    \vX &=
    \footnotesize\begin{bmatrix}
      1 & 0 & 0 & 1 & 0 & 1 \\ 
      1 & 0 & 0 & 0 & 0 & 1 \\ 
      0 & 0 & 0 & 0 & 0 & 1 \\ 
      0 & 0 & 0 & 1 & 0 & 1 \\ 
      0 & 1 & 0 & 0 & 0 & 1 \\ 
      1 & 0 & 0 & 0 & 1 & 1 \\ 
      0 & 1 & 0 & 1 & 1 & 1 \\ 
      0 & 0 & 1 & 0 & 0 & 1 \\      
      1 & 0 & 0 & 1 & 0 & 1 \\ 
      1 & 0 & 0 & 1 & 0 & 1
    \end{bmatrix}
    &
    \vz &=
    \footnotesize\begin{bmatrix}
      1  \\
      0  \\
      0  \\
      1  \\
      1  \\
      0  \\
      1  \\
      0  \\
      1  \\
      1 
    \end{bmatrix}
    &
    \vw &=
    \begin{bmatrix}
      V\psup{1} \\
      \vdots \\
      V\psup{n}
    \end{bmatrix}
  \end{align*}
  
  \begin{align*}
    \only<beamer:1-3| handout:0>{
    \visible<2->{
    \small\begin{bmatrix} 
      3 \\ 
      2 \\
      0 \\
      5 \\
      1 \\
      6
    \end{bmatrix}
    &= \vX^T \vz}
    &
    \visible<3->{
    \small\begin{bmatrix} 
      5 & 0 & 0 & 3 & 1 &  5 \\ 
      0 & 2 & 0 & 1 & 1 &  2 \\ 
      0 & 0 & 1 & 0 & 0 &  1 \\ 
      3 & 1 & 0 & 5 & 1 &  5 \\ 
      1 & 1 & 0 & 1 & 2 &  2 \\ 
      5 & 2 & 1 & 5 & 2 & 10 
    \end{bmatrix}
    &= \vX^T \vX}
    }%
    \only<beamer:4-| handout:1>{
    \small\begin{bmatrix} 
      .3 \\ 
      .2 \\
      .0 \\
      .5 \\
      .1 \\
      .6
    \end{bmatrix}
    &= \tfrac{1}{m} \vX^T \vz
    &
    \small\begin{bmatrix} 
      .5 & .0 & .0 & .3 & .1 &  .5 \\ 
      .0 & .2 & .0 & .1 & .1 &  .2 \\ 
      .0 & .0 & .1 & .0 & .0 &  .1 \\ 
      .3 & .1 & .0 & .5 & .1 &  .5 \\ 
      .1 & .1 & .0 & .1 & .2 &  .2 \\ 
      .5 & .2 & .1 & .5 & .2 &  1 
    \end{bmatrix}
    &= \tfrac{1}{m} \vX^T \vX
    }
  \end{align*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NDL vs.\ the Perceptron vs.\  least-squares regression}

\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle}}
\tikzset{functions/.style={basic,circle,fill=faugold!10}}
\tikzset{basic/.style={draw,fill=faublue!10,text width=1em,text badly centered}}
%% \tikzset{flow/.style={draw,-{>[scale=1.2]}}} % nicer, but only works with PGF 3.0
\tikzset{flow/.style={draw,->}} % nicer, but only works with PGF 3.0

\begin{frame}
  \frametitle{The single-layer perceptron (SLP)}
  %% \framesubtitle{}

  \begin{columns}[c]
    \begin{column}{6cm}
      SLP \citep{Rosenblatt:58} is most basic feed-forward \primary{neural network}
      \begin{itemize}
      \item<1-> numeric inputs $x_1, \ldots, x_n$
      \item<1-> output activation $h(y)$ based on weighted sum of inputs
        \[
        y = \textstyle\sum_{j=1}^n w_j x_j
        \]
      \item<2-> $h$ = Heaviside step function in traditional SLP
      \item<3-> even simpler model: $h(y) = y$
      \item<4-> cost wrt.\ target output $z$:
        \[
        E(\vw, \vx, z) = \left( z - \textstyle\sum_{j=1}^n w_j x_j \right)^2
        \]
      \end{itemize}
    \end{column}
    \begin{column}{5cm}
      \begin{tikzpicture}
        \node[functions] (center) {};
        \node[below=0.5em of center,font=\scriptsize,text width=3.4em] {activation function $h$};
        \draw (0em,0.75em) -- (0em,-0.75em);
        \draw (0.75em,0em) -- (-0.75em,0em);
        \only<beamer:1-2| handout:0>{
          \draw[very thick, color=primary] (0.7em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.7em,-0.5em);
        }
%%        \draw[very thick, color=primary] (0.7em,0.5em) .. controls (-0.2em,0.5em) and (0.2em,-0.5em) .. (-0.7em,-0.5em);
        \only<beamer:3-| handout:1>{
          \draw[very thick, color=primary] (0.6em,0.6em) -- (-0.6em,-0.6em);
        }
        \node[right of=center] (right) {};
        \path[flow] (center) -- (right);
        \node[functions,left=1.5em of center] (left) {$\sum$};
        \path[flow] (left) -- (center);
        \node[weights,above left=0.5em and 2em of left] (2) {$w_2$} -- (2) node[input,left=1em of 2] (l2) {$x_2$};
        \path[flow] (l2) -- (2);
        \path[flow] (2) -- (left);
        \node[below of=2] (dots) {$\vdots$} -- (dots) node[below of=l2] (ldots) {$\vdots$};
        \node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left=1em of n] (ln) {$x_n$};
        \path[flow] (ln) -- (n);
        \path[flow] (n) -- (left);
        \node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left=1em of 1] (l1) {$x_1$};
        \path[flow] (l1) -- (1);
        \path[flow] (1) -- (left);
        \node[below=1.5em of ln.center,font=\scriptsize] {inputs};
        \node[below=1.5em of n.center,font=\scriptsize] {weights};
      \end{tikzpicture}
    \end{column}
  \end{columns}  
\end{frame}

\begin{frame}
  \frametitle{SLP training: the delta rule}
  %% \framesubtitle{}
  \begin{itemize}
  \item SLP weights are learned by \primary{gradient descent} training:\\
    for a single training item $(\vx, z)$ and learning rate $\delta > 0$
    \begin{align*}
      \Delta w_i &= -\delta \frac{\partial E(\vw, \vx, z)}{\partial w_i} \\
      \only<beamer:1-2| handout:0>{\visible<2->{&= -\delta \frac{\partial}{\partial w_i} \left( z - \sum_{j=1}^n w_j x_j \right)^2 \\}}
      \only<beamer:3-4| handout:0>{&= -2\delta \left( z - \sum_{j=1}^n w_j x_j \right) (-x_i) \\}
      \only<beamer:5-| handout:1>{&= 2\delta x_i \left( z - \sum_{j=1}^n x_ j w_j \right) \\}
      \visible<4->{&= \secondary{\beta c_i \bigl( o - \textstyle\sum_{j=1}^n c_j V_j \bigr)}}
    \end{align*}
  \item<6-> Perfect \primary{correspondence to W-H rule} with
    \[
    V_i = w_i \qquad c_i = x_i \qquad o = z \qquad \beta = 2\delta
    \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Batch training}
  %% \framesubtitle{}

  \begin{itemize}
  \item Neural networks often use \primary{batch training}, where all training data are considered at once instead of one item at a time
  \item The corresponding batch training cost is
    \begin{align*}
    E(\vw) &= \frac{1}{m} \sum_{k=1}^m E(\vw, \vx[k], z\psup{k}) \\
    \visible<3->{ &= \frac{1}{m} \sum_{k=1}^m \left( z\psup{k} - \textstyle\sum_{j=1}^n w_j x_j\psup{k} \right)^2 }
    \end{align*}
  \item<2-> Similar to stochastic NDL, batch training computes the expected weights $\bigExp{\vw\psupt}$ for an SLP with stochastic input
  \item<3-> Minimization of $E(\vw)$ = linear \primary{least-squares regression}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Linear least-squares regression}
  %% \framesubtitle{}
  
  \begin{itemize}
  \item Matrix formulation of the linear least-squares problem:
    \begin{align*}
      E(\vw) &= \frac{1}{m} \sum_{k=1}^m \left( z\psup{k} - \textstyle\sum_{j=1}^n w_j x_j\psup{k} \right)^2 \\
      \visible<2->{ &= \frac{1}{m} \bigl( \vz - \vX \vw \bigr)^T \bigl( \vz - \vX \vw \bigr) }
    \end{align*}
  \item<3-> Minimum of $E(\vw)$, the $L_2$ solution, must satisfy $\nabla E(\vw^*) = \vnull$, which leads to the \primary{normal equations}
    \[
    \vX^T \vz = \vX^T \vX \vw^*
    \]
  \item<4-> Normal equations = Danks equilibrium conditions
  \item<5-> Regression theory shows that batch training / stochastic NLP converges to the unique$^{\primary{*}}$ solution of the $L_2$ problem
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{What have we learned?}
  %% \framesubtitle{}

  \begin{center}\Large
    \setlength{\fboxrule}{2pt}
    \fcolorbox{secondary}{faugold!10!white}{
      \begin{tabular}{c c c c c}
        stochastic &=& batch &=& $L_2$ regression \\[1ex]
        NDL &=& SLP
      \end{tabular}
    }
  \end{center}
  \begin{itemize}
  \item[\hand] These equivalences also hold for the general R-W equations with arbitrary values of $\alpha_i$, $\beta_1$, $\beta_2$ and $\lambda$ (see paper)
  \end{itemize}
  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 3 Insights
\section{Insights}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Theoretical insights}

\begin{frame}
  \frametitle{Effects of R-W parameters}
  %% \framesubtitle{}

  \begin{description}
  \item<1->[$\beta > 0$:] learning rate \so convergence of individual learners
  \item<2->[$\lambda \neq 1$:]\gap[.5] linear scaling of associations / activation (obvious)
  \item<3->[$\alpha_i\neq 1$:]\gap[.5] salience of cue $C_i$ determines how fast associations are learned, but does not affect the final stable associations (same $L_2$ regression problem)
  \item<4->[$\beta_1 \neq \beta_2$:]\gap[.5] different positive/negative learning rates \emph{do} affect the stable associations; closely related to prevalence of positive and negative events in the population
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{What about logistic regression?}
  %% \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{6cm}
      Logistic regression is the standard tool for predicting a categorical response from binary features 
      \begin{itemize}
      \item<1-> can be expressed as SLP with probabilistic interpretation
      \item<2-> uses logistic activation function
        \[
        h(y) = \frac{1}{1 + e^{-y}}
        \]
      \item<3-> and Bernoulli cost
        \[
        E(\vw, \vx, z) = \begin{cases}
          -\log h(y) & \text{if } z = 1 \\
          -\log (1 - h(y)) & \text{if } z = 0
        \end{cases}
        \]
      \end{itemize}
    \end{column}
    \begin{column}{5cm}
      \begin{tikzpicture}
        \node[functions] (center) {};
        \node[below=0.5em of center,font=\scriptsize,text width=3.4em] {activation function $h$};
        \draw (0em,0.75em) -- (0em,-0.75em);
        \draw (0.75em,0em) -- (-0.75em,0em);
        \only<beamer:1| handout:0>{
          \draw[very thick, color=primary] (0.6em,0.6em) -- (-0.6em,-0.6em);
        }
        \only<beamer:2-| handout:1>{
          \draw[very thick, color=primary] (0.7em,0.5em) .. controls (-0.2em,0.5em) and (0.2em,-0.5em) .. (-0.7em,-0.5em);
        }
        \node[right of=center] (right) {};
        \path[flow] (center) -- (right);
        \node[functions,left=1.5em of center] (left) {$\sum$};
        \path[flow] (left) -- (center);
        \node[weights,above left=0.5em and 2em of left] (2) {$w_2$} -- (2) node[input,left=1em of 2] (l2) {$x_2$};
        \path[flow] (l2) -- (2);
        \path[flow] (2) -- (left);
        \node[below of=2] (dots) {$\vdots$} -- (dots) node[below of=l2] (ldots) {$\vdots$};
        \node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left=1em of n] (ln) {$x_n$};
        \path[flow] (ln) -- (n);
        \path[flow] (n) -- (left);
        \node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left=1em of 1] (l1) {$x_1$};
        \path[flow] (l1) -- (1);
        \path[flow] (1) -- (left);
        \node[below=1.5em of ln.center,font=\scriptsize] {inputs};
        \node[below=1.5em of n.center,font=\scriptsize] {weights};
      \end{tikzpicture}
    \end{column}
  \end{columns}  
\end{frame}

\begin{frame}
  \frametitle{What about logistic regression?}
  %% \framesubtitle{}

  \begin{itemize}
  \item Gradient descent training leads to delta rule that corresponds to a modified version of the R-W equations
    \[
    \Delta V_i =
    \begin{cases}
      0 & \text{if } c_i = 0\\
      \beta \left( 1 - h \bigl( \sum_{j=1}^n c_j V_j \bigr) \right) & \text{if } c_i = 1 \wedge o = 1 \\
      \beta \left( 0 - h \bigl( \sum_{j=1}^n c_j V_j \bigr) \right) & \text{if } c_i = 1 \wedge o = 0
    \end{cases}
    \]
  \item<2-> Same as original R-W, except that activation level is now transformed into probability $h(y)$
  \item<2-> But no easy way to analyze stochastic learning process\\
    (batch training $\neq$ expected value of single-item training)
  \item<2-> Less robust for highly predictable outcomes \so $\vw$ diverges
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Empirical observations}

\begin{frame}
  \frametitle{Some NDL simulation runs}

  \centering
  \only<beamer:1-7| handout:0>{\ungap[2]}%
  \only<beamer:1| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.AgentGroup_pohtia_RW_vs_D}}}}%
  \only<beamer:2| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PersonFirst_miettia_RW_vs_D}}}}%
  \only<beamer:3| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PersonFirst_pohtia_RW_vs_D}}}}%
  \only<beamer:4| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PatientInfinitive_ajatella_RW_vs_D}}}}%
  \only<beamer:5| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PatientInfinitive_ajatella_RW_vs_Dx5}}}}%
  \only<beamer:6| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PatientDirectQuote_ajatella_RW_vs_D}}}}%
  \only<beamer:7| handout:0>{\includegraphics[width=7.5cm]{{{img/think.qitl2.PatientDirectQuote_ajatella_RW_vs_Dx5}}}}%
  \only<beamer:1-7| handout:0>{\\ \ungap[.5]\color{secondary}}%
  \only<beamer:1| handout:0>{moderate positive association \so convergence}%
  \only<beamer:2| handout:0>{equivocal association \so convergence}%
  \only<beamer:3| handout:0>{equivocal association \so convergence}%
  \only<beamer:4| handout:0>{near-perfect positive association \so non-convergence with $1\times$ data}%
  \only<beamer:5| handout:0>{near-perfect positive association \so convergence with $5\times$ data}%
  \only<beamer:6| handout:0>{near-perfect negative association \so non-convergence with $1\times$ data}%
  \only<beamer:7| handout:0>{near-perfect negative association \so convergence with $5\times$ data}%
  \only<beamer:0| handout:1>{%
    \begin{tabular}{ccc}
      \includegraphics[width=3.5cm]{{{img/think.qitl2.AgentGroup_pohtia_RW_vs_D}}} &
      \includegraphics[width=3.5cm]{{{img/think.qitl2.PersonFirst_miettia_RW_vs_D}}} &
      \includegraphics[width=3.5cm]{{{img/think.qitl2.PatientInfinitive_ajatella_RW_vs_D}}} \\
      \includegraphics[width=3.5cm]{{{img/think.qitl2.PatientInfinitive_ajatella_RW_vs_Dx5}}} &
      \includegraphics[width=3.5cm]{{{img/think.qitl2.PatientDirectQuote_ajatella_RW_vs_D}}} &
      \includegraphics[width=3.5cm]{{{img/think.qitl2.PatientDirectQuote_ajatella_RW_vs_Dx5}}}
    \end{tabular}    
  }%
\end{frame}


\begin{frame}
  \frametitle{Convergence \vs non-convergence -- artificial data}

  \begin{center}
    \begin{tabular}{ l r r >{\color{secondary}}r }
      \toprule
      word form & frequency &   outcomes &  \foreground{cues} \\
      \midrule
      hand &       10 &    \counterpoint{hand}\_NIL &   h\_a\_n\_d \\
      hands &       20 & \counterpoint{hand}\_\primary{PLURAL} & h\_a\_n\_d\_s \\
      land &        8 &    land\_NIL &   l\_a\_n\_d \\
      lands &        3 & land\_\primary{PLURAL} & l\_a\_n\_d\_s \\
      and &       35 &     and\_NIL &     a\_n\_d \\
      sad &       18 &     sad\_NIL &     s\_a\_d \\
      as &       35 &      \fourth{as}\_NIL &       a\_s \\
      lad &      102 &     lad\_NIL &     l\_a\_d \\
      lad &       54 &  lad\_\primary{PLURAL} &     l\_a\_d \\
      lass &      134 &    lass\_NIL &   l\_a\_s\_s \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Perfect positive association \so convergence}

  \centering
  \includegraphics[width=8cm]{{{img/plurals_h_hand}}}
\end{frame}

\begin{frame}[c]
  \frametitle{Moderate positive association \so non-convergence}

  \centering
  \includegraphics[width=8cm]{{{img/plurals_s_PLURAL}}}
\end{frame}

\begin{frame}[c]
  \frametitle{Perfect positive association \so convergence}

  \centering
  \includegraphics[width=8cm]{{{img/plurals_a_as}}}
\end{frame}

\begin{frame}[c]
  \frametitle{Moderate negative association \so non-convergence}

  \centering
  \includegraphics[width=8cm]{{{img/plurals_s_as}}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conclusion}

\begin{frame}
  \frametitle{Summary \& next steps}
  %% \framesubtitle{}
  \begin{center}
    \setlength{\fboxrule}{1pt}
    \fcolorbox{secondary}{faugold!10!white}{
      \begin{tabular}{c c c c c}
        stochastic &=& batch &=& $L_2$ regression \\[1ex]
        NDL &=& SLP
      \end{tabular}
    }
  \end{center}
  
  \begin{itemize}
  \item<2-> How many training steps are needed for a stochastic NDL learner to
    converge to the Danks equilibrium?
  \item<3-> Are there cases of non-convergence? If yes, why?
  \item<4-> Does NDL accuracy always improve with more cues and more training data?
      If not, why?
  \item<5-> How does logistic regression behave as incremental learner?
  \item<6-> Which sequences / patterns in the input data lead to significantly
    different behaviour from stochastic learner?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Acknowledgements 1/2}
  %% \framesubtitle{}

  \ungap[1]
  \begin{columns}[T]
    \begin{column}{55mm}
      \includegraphics[width=52mm]{img/ratti_cinnamon_rolls}
      
      \scriptsize
      Follow me on Twitter: \secondary{@RattiTheRat}
    \end{column}
    \begin{column}{50mm}
      The mathematical analysis was fuelled by large amounts of coffee and cinnamon rolls at Cinnabon, Harajuku, Tokyo

    \end{column}

  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Acknowledgements 2/2}
  %% \framesubtitle{}

  \ungap[1]
  \begin{columns}[T]
    \begin{column}{70mm}
      \includegraphics[width=68mm]{img/Cottage_on_Saaremaa}
    \end{column}
    \begin{column}{40mm}
      The empirical analyses were conducted in the natural environment of Ninase, Saaremaa, Estonia.
    \end{column}

  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References (if any)

\frame[allowframebreaks]{
  \frametitle{References}
  \bibliographystyle{natbib-stefan}
  \begin{scriptsize}
    \bibliography{references}
  \end{scriptsize}
}

\end{document}
