\begin{frame}
  \frametitle{Objectives}

  \begin{itemize}

  \item Present the mathematic underpinnings of NDL in one place, in a systematic way

  \item High-light the theoretical similarities of NDL with linear/logistic regression and perceptron

  \item Present some empirical simulations of NDL, in light of the theory
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Naive Discriminative Learning}
  
  \begin{itemize}
  
  \item Baayen et al. 2011; Baayen 2011

  \item Rescorla-Wagner (1972) incremental learning equations

  \item Danks (2003) equilibrium equations

  \item Implementation as an R package \texttt{ndl}: Arppe et
    al. 2011; Shaoul et al. 2013

\end{itemize}    

\end{frame}

\begin{frame}
\frametitle{Rescorla-Wagner equations (1972) -- verbally}

Represent incremental learning and subsequently on-going adjustments
to an accumulating body of knowledge:

Changes in association strengths:
\begin{itemize}
	\item If a cue is not present in the input, no change
	\item Increased when the cue and outcome co-occur
	\item Decreased when the cue occurs without the outcome
	\item The more cues are present simultaneously, the smaller the adjustments are 
\end{itemize}

Only the results of the incremental adjustments to the cue-outcome
associations are kept -- no need for remembering the individual
adjustments, however many there are.

\end{frame}

\begin{frame}
  \frametitle{Danks (2003) equilibrium equations -- verbally} 

  \begin{itemize}

  \item presume an ideal `adult/stable' state where all the
    cue-outcome associations have been fully learnt -- any more data
    points bring nothing `new' to learn, i.e. have zero impact on the
    cue-outcome associations.
    
  \item make it possible to estimate the weights for a system using
    the co-occurrence vector of a specific outcome given the different
    cue (predictor) values and the co-occurrence matrix of cue
    (predictor) values -- using relatively simple matrix algebra.

  \item provide a convenient short-cut to calculating the
    consolidated cue-outcome association weights resulting from
    incremental learning.

  \item the learning parameters of the Rescorla-Wagner equations drop
    out of the equilibrium equations.

  \item circumvent the problem that a simulation of an Rescorla-Wagner
    learner does not converge to a single state unless the learning
    rate is gradually decreased.
    
\end{itemize}

\end{frame}
 
\begin{frame}
  \frametitle{Naive Discriminative Learning}

  \begin{itemize}

  \item Naive: cue-outcome associations estimatated separately for
    each outcome (this simplifying assumption of independence similar to
    a naive Bayesian classifier).

  \item Discriminative: direct associations with each outcome given a set of cues.

  \item Learning: based on incremental learning.

  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Rescorla-Wagner equations -- traditional vs. linguistic
    applications}

  \begin{itemize}

  \item traditionally: simple controlled experiments on item-by-item
    learning, with only a couple of cues and some perfect associations.

  \item natural language: full of choices among multiple possible
    alternatives -- phones, words, or constructions -- which are
    influenced by a large number of contextual factors, and which
    rather exhibit asymptotic, imperfect tendencies favoring one or
    more of the alternatives, instead of single, categorical, perfect
    choices.

  \item We find these messy, complex types of problems as a key area
    of interest in modeling and understanding language use.

  \item We consequently consider the application of the
    Rescorla-Wagner equations in the form of a Naive Discriminative
    Learning classifier to such linguistic phenomena of considerable
    utility.
    
\end{itemize}

\end{frame}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../qitl6_evert_arppe"
%%% End: 
